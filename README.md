# Web-Crawler

ğŸ›¡ï¸ WebCrawler Recon CLI

Mapeamento automatizado da superfÃ­cie de exposiÃ§Ã£o de aplicaÃ§Ãµes web

Ferramenta de reconhecimento de seguranÃ§a (recon) desenvolvida em Python para identificar informaÃ§Ãµes expostas, endpoints visÃ­veis e ativos esquecidos em aplicaÃ§Ãµes web pÃºblicas.

Este projeto foi criado com foco defensivo, simulando o primeiro passo de um atacante real, porÃ©m com o objetivo de prevenir incidentes antes que eles aconteÃ§am.

# ğŸ¯ Problema que resolve

Na maioria das empresas, a pergunta nÃ£o Ã© â€œtemos vulnerabilidades?â€
A pergunta real Ã©:

O que nossa aplicaÃ§Ã£o estÃ¡ expondo hoje sem perceber?

Quais endpoints, APIs e dados estÃ£o visÃ­veis publicamente?

Nossa superfÃ­cie de ataque estÃ¡ realmente mapeada?

O que um atacante veria em 5 minutos de reconhecimento?

Este projeto existe para responder essas perguntas rapidamente, de forma automatizada e auditÃ¡vel.

# ğŸ§  O que a ferramenta faz (visÃ£o executiva)

O WebCrawler Recon CLI realiza um mapeamento controlado da aplicaÃ§Ã£o web, coletando informaÃ§Ãµes que frequentemente passam despercebidas em produÃ§Ã£o, como:

Dados sensÃ­veis expostos acidentalmente

Endpoints de API visÃ­veis no frontend

ComentÃ¡rios HTML esquecidos (debug, TODOs, chaves)

SubdomÃ­nios referenciados fora do inventÃ¡rio oficial

Estrutura real de links internos da aplicaÃ§Ã£o

Tudo isso sem explorar vulnerabilidades, apenas analisando o que jÃ¡ estÃ¡ pÃºblico â€” exatamente como um atacante faria antes de decidir atacar.

# ğŸ” O que ele identifica na prÃ¡tica

## ğŸ“§ E-mails expostos

## â˜ï¸ Telefones em pÃ¡ginas pÃºblicas

## ğŸ”— Endpoints de API (/api/...)

## ğŸ” Tokens comuns (JWT, API keys simples)

## ğŸ’¬ ComentÃ¡rios HTML esquecidos

## ğŸŒ SubdomÃ­nios referenciados

## ğŸ§­ Mapa de links internos

Os resultados sÃ£o entregues em JSON estruturado, pronto para anÃ¡lise, auditoria ou integraÃ§Ã£o com outros sistemas.

âš™ï¸ Diferencial tÃ©cnico (o que mostra maturidade)

O diferencial nÃ£o Ã© apenas rastrear pÃ¡ginas, mas como isso Ã© feito:

Controle de profundidade (evita ruÃ­do e bloqueios)

Delay configurÃ¡vel (respeita ambientes produtivos)

Interface CLI (automatizÃ¡vel e versionÃ¡vel)

Output estruturado (integraÃ§Ã£o com CI/CD, SIEM, auditorias)

CÃ³digo simples, auditÃ¡vel e fÃ¡cil de estender

Isso reflete pensamento de engenharia e seguranÃ§a, nÃ£o apenas conhecimento de linguagem.

# ğŸ§© Onde isso se encaixa no negÃ³cio

# Essa ferramenta pode ser usada em:

## ğŸ” Pipelines CI/CD (prÃ©-deploy)

## ğŸ” Auditorias internas de seguranÃ§a

## ğŸ§± Hardening de aplicaÃ§Ãµes web

## ğŸ“‹ InventÃ¡rio contÃ­nuo de exposiÃ§Ã£o

## ğŸ›¡ï¸ Times de AppSec / DevSecOps

O ganho real Ã©:

ReduÃ§Ã£o de risco

Menos surpresas em auditorias

Menos incidentes causados por descuido

ğŸš€ Uso bÃ¡sico
python crawler.py -u https://example.com


Com mais controle:

python crawler.py -u https://example.com -d 3 --delay 2 -o output/scan.json

# ğŸ“¤ Exemplo de output
**{
  "target": "https://example.com",
  "emails": ["admin@example.com"],
  "telefones": [],
  "endpoints": ["/api/login"],
  "tokens": [],
  "comentarios": ["TODO: remover token"],
  "subdominios": ["dev.example.com"],
  "urls_visitadas": 14
}**

ğŸ”’ ConsideraÃ§Ãµes de seguranÃ§a

A ferramenta nÃ£o realiza exploraÃ§Ã£o

NÃ£o faz brute-force

NÃ£o contorna proteÃ§Ãµes

Atua apenas sobre informaÃ§Ãµes publicamente acessÃ­veis

Ideal para uso responsÃ¡vel, interno e preventivo.


